# -*- coding: utf-8 -*-
import os
import time
import math
import json
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta, timezone
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from wordcloud import WordCloud
from io import BytesIO
from PIL import Image
import folium
from folium.plugins import TimestampedGeoJson
from branca.colormap import linear

# =========================
# CONFIG & CONSTANTS
# =========================
st.set_page_config(page_title="Hanoi Air Quality: 30-day Analysis", layout="wide")

# B·∫°n c√≥ th·ªÉ thay b·∫±ng bi·∫øn m√¥i tr∆∞·ªùng n·∫øu mu·ªën: os.environ.get("OWM_API_KEY")
OWM_API_KEY = "490f66c05505839fe0646bb5aa5770dc"
CITY_NAME = "Hanoi"
COUNTRY_CODE = "VN"
TARGET_DAYS = 30
CHUNK_DAYS = 5  # OWM free th∆∞·ªùng ch·ªâ cho 5 ng√†y history/ƒë·ª£t
TIMEZONE = timezone(timedelta(hours=7))  # Asia/Bangkok (UTC+7)

# =========================
# UTILS
# =========================
@st.cache_data(show_spinner=False, ttl=3600)
def geocode_city(city, country, api_key):
    url = "https://api.openweathermap.org/geo/1.0/direct"
    params = {"q": f"{city},{country}", "limit": 1, "appid": api_key}
    r = requests.get(url, params=params, timeout=30)
    r.raise_for_status()
    js = r.json()
    if not js:
        raise ValueError("Kh√¥ng t√¨m th·∫•y to·∫° ƒë·ªô th√†nh ph·ªë.")
    lat, lon = js[0]["lat"], js[0]["lon"]
    return lat, lon, js[0]

def unix(dt):
    return int(dt.replace(tzinfo=timezone.utc).timestamp())

def to_local(ts):
    # OWM tr·∫£ dt l√† UNIX UTC
    return datetime.fromtimestamp(int(ts), tz=timezone.utc).astimezone(TIMEZONE)

@st.cache_data(show_spinner=True, ttl=1800)
def get_air_history(lat, lon, start_dt, end_dt, api_key):
    """
    G·ªçi OWM Air Pollution History theo c√°c c·ª≠a s·ªï 5 ng√†y ƒë·ªÉ c·ªë l·∫•y ƒë·ªß 30 ng√†y.
    Tr·∫£ v·ªÅ list b·∫£n ghi (dict) t·ª´ API.
    """
    collected = []
    total_days = (end_dt - start_dt).days + 1
    # l√πi theo c√°c khung 5 ng√†y
    window_end = end_dt
    while window_end > start_dt:
        window_start = max(start_dt, window_end - timedelta(days=CHUNK_DAYS-1))
        url = "https://api.openweathermap.org/data/2.5/air_pollution/history"
        params = {
            "lat": lat, "lon": lon,
            "start": unix(window_start.replace(hour=0, minute=0, second=0, microsecond=0)),
            "end": unix((window_end + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)),
            "appid": api_key
        }
        try:
            r = requests.get(url, params=params, timeout=45)
            if r.status_code == 429:
                # h·∫°n m·ª©c -> ngh·ªâ 2s r·ªìi th·ª≠ l·∫°i
                time.sleep(2)
                r = requests.get(url, params=params, timeout=45)
            r.raise_for_status()
            js = r.json()
            if isinstance(js, dict) and "list" in js:
                collected.extend(js["list"])
        except requests.HTTPError as e:
            # N·∫øu v∆∞·ª£t gi·ªõi h·∫°n l·ªãch s·ª≠ (free), v·∫´n ti·∫øp t·ª•c v·ªõi ph·∫ßn l·∫•y ƒë∆∞·ª£c
            st.info(f"‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c ƒëo·∫°n {window_start.date()} ‚Üí {window_end.date()}: {e}")
        window_end = window_start - timedelta(days=1)
    # lo·∫°i tr√πng theo timestamp
    seen = set()
    uniq = []
    for it in collected:
        t = int(it.get("dt", -1))
        if t not in seen and t != -1:
            uniq.append(it)
            seen.add(t)
    # sort theo th·ªùi gian
    uniq.sort(key=lambda x: x.get("dt", 0))
    return uniq

def normalize_dataframe(raw_list, lat, lon):
    if not raw_list:
        return pd.DataFrame()
    rows = []
    for it in raw_list:
        dt_utc = int(it.get("dt", 0))
        main = it.get("main", {}) or {}
        comps = it.get("components", {}) or {}
        rows.append({
            "dt_utc": dt_utc,
            "time": to_local(dt_utc),
            "aqi": main.get("aqi", np.nan),
            "co": comps.get("co", np.nan),
            "no": comps.get("no", np.nan),
            "no2": comps.get("no2", np.nan),
            "o3": comps.get("o3", np.nan),
            "so2": comps.get("so2", np.nan),
            "pm2_5": comps.get("pm2_5", np.nan),
            "pm10": comps.get("pm10", np.nan),
            "nh3": comps.get("nh3", np.nan),
            "lat": lat, "lon": lon
        })
    df = pd.DataFrame(rows).drop_duplicates(subset=["dt_utc"]).sort_values("time").reset_index(drop=True)

    # Chu·∫©n ho√° ki·ªÉu d·ªØ li·ªáu
    num_cols = ["aqi","co","no","no2","o3","so2","pm2_5","pm10","nh3","lat","lon"]
    for c in num_cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")

    # X·ª≠ l√Ω thi·∫øu: forward-fill theo gi·ªù (v√¨ c√πng 1 ƒëi·ªÉm ƒëo)
    if not df.empty:
        df = df.set_index("time")
        df[num_cols] = df[num_cols].interpolate(method="time", limit_direction="both")
        df = df.reset_index()

    # T·∫°o ƒë·∫∑c tr∆∞ng m·ªõi
    if not df.empty:
        df["pm_ratio"] = df["pm2_5"] / df["pm10"]
        df["hour"] = df["time"].dt.hour
        df["day"] = df["time"].dt.date.astype(str)
        df["weekday"] = df["time"].dt.day_name()
        df["is_weekend"] = df["weekday"].isin(["Saturday","Sunday"]).astype(int)
        df["rolling_pm25_24h"] = df["pm2_5"].rolling(24, min_periods=6).mean()
        df["rolling_pm10_24h"] = df["pm10"].rolling(24, min_periods=6).mean()
        df["aqi_label"] = df["aqi"].map({
            1:"Good", 2:"Fair", 3:"Moderate", 4:"Poor", 5:"Very Poor"
        }).fillna("Unknown")
    return df

def corr_df(df):
    cols = ["aqi","co","no","no2","o3","so2","pm2_5","pm10","nh3","pm_ratio","rolling_pm25_24h","rolling_pm10_24h"]
    cols = [c for c in cols if c in df.columns]
    return df[cols].corr().round(3)

def summarize_insight(df):
    if df.empty:
        return {
            "n": 0, "date_range": "No data",
            "max_pm25": None, "max_pm25_time": None,
            "bad_hours_share": None,
            "mean_pm25": None, "median_pm25": None,
            "top_day": None
        }
    # Th·ªëng k√™ c∆° b·∫£n
    n = len(df)
    dr = f"{df['time'].min().date()} ‚Üí {df['time'].max().date()}"
    idx_max = df["pm2_5"].idxmax()
    time_max = df.loc[idx_max, "time"]
    max_pm25 = df.loc[idx_max, "pm2_5"]
    bad_hours_share = (df["aqi"] >= 4).mean()  # t·ªâ l·ªá gi·ªù 'Poor' & 'Very Poor'
    mean_pm25 = df["pm2_5"].mean()
    median_pm25 = df["pm2_5"].median()
    daily = df.groupby("day")["pm2_5"].mean().sort_values(ascending=False)
    top_day = daily.index[0] if len(daily) else None
    return {
        "n": n, "date_range": dr,
        "max_pm25": float(max_pm25), "max_pm25_time": time_max.strftime("%Y-%m-%d %H:%M"),
        "bad_hours_share": float(bad_hours_share),
        "mean_pm25": float(mean_pm25), "median_pm25": float(median_pm25),
        "top_day": top_day
    }

def make_wordcloud(text_series):
    txt = " ".join([str(x) for x in text_series if pd.notna(x)])
    if not txt.strip():
        return None
    wc = WordCloud(width=1200, height=600, background_color="white").generate(txt)
    bio = BytesIO()
    wc.to_image().save(bio, format="PNG")
    bio.seek(0)
    return Image.open(bio)

# =========================
# SIDEBAR
# =========================
st.sidebar.title("‚öôÔ∏è Controls")
st.sidebar.write("Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng kh√¥ng kh√≠ t·∫°i **H√† N·ªôi** (OWM).")
target_days = st.sidebar.slider("S·ªë ng√†y g·∫ßn nh·∫•t", 7, 30, TARGET_DAYS, 1)
show_regression = st.sidebar.checkbox("Scatter + OLS regression", True)
use_treemap = st.sidebar.selectbox("Ph√¢n c·∫•p", ["Sunburst AQI ‚Üí Weekday", "Treemap AQI ‚Üí Hour"], index=0)
map_mode = st.sidebar.selectbox("B·∫£n ƒë·ªì", ["Folium (Timestamped)", "Plotly Mapbox"], index=0)

# =========================
# DATA FETCH
# =========================
st.title("üå´Ô∏è Hanoi Air Quality ‚Äî 30-day Analysis (OpenWeatherMap)")
st.caption("Ngu·ªìn: OpenWeatherMap Air Pollution API. ·ª®ng d·ª•ng s·∫Ω t·ª± b√°o n·∫øu ch·ªâ l·∫•y ƒë∆∞·ª£c ‚â§5 ng√†y do gi·ªõi h·∫°n g√≥i.")

# Geocode
lat, lon, geo_meta = geocode_city(CITY_NAME, COUNTRY_CODE, OWM_API_KEY)

# Time window
end_local = datetime.now(tz=TIMEZONE)
start_local = end_local - timedelta(days=target_days-1)
# OWM history d√πng UTC
start_utc = start_local.astimezone(timezone.utc).replace(minute=0, second=0, microsecond=0)
end_utc = end_local.astimezone(timezone.utc).replace(minute=0, second=0, microsecond=0)

raw = get_air_history(lat, lon, start_utc, end_utc, OWM_API_KEY)
df = normalize_dataframe(raw, lat, lon)

if df.empty:
    st.error("Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu t·ª´ OWM. H√£y ki·ªÉm tra API key/quy·ªÅn truy c·∫≠p ho·∫∑c th·ª≠ l·∫°i sau.")
    st.stop()

# C·∫£nh b√°o n·∫øu ƒë·ªô ph·ªß < 30 ng√†y
got_days = (df["time"].max().date() - df["time"].min().date()).days + 1
if got_days < target_days - 1:
    st.warning(f"Ch·ªâ l·∫•y ƒë∆∞·ª£c ~{got_days} ng√†y d·ªØ li·ªáu (gi·ªõi h·∫°n API). V·∫´n ti·∫øn h√†nh ph√¢n t√≠ch tr√™n ph·∫ßn d·ªØ li·ªáu n√†y.")

# =========================
# SUMMARY KPIs & STORYTELLING
# =========================
kpi = summarize_insight(df)
c1, c2, c3, c4 = st.columns(4)
c1.metric("Kho·∫£ng th·ªùi gian", kpi["date_range"])
c2.metric("S·ªë b·∫£n ghi (gi·ªù)", f"{kpi['n']:,}")
c3.metric("PM2.5 cao nh·∫•t (¬µg/m¬≥)", f"{kpi['max_pm25']:.1f}" if kpi["max_pm25"] else "‚Äî", help=f"Th·ªùi ƒëi·ªÉm: {kpi['max_pm25_time'] or '‚Äî'}")
c4.metric("T·ªâ l·ªá gi·ªù AQI x·∫•u (‚â•Poor)", f"{kpi['bad_hours_share']*100:.1f}%" if kpi["bad_hours_share"] is not None else "‚Äî")

with st.expander("üßæ 1 trang storytelling (insights ch√≠nh)"):
    st.markdown(f"""
**B·ªëi c·∫£nh.** Ph√¢n t√≠ch chu·ªói th·ªùi gian ch·∫•t l∆∞·ª£ng kh√¥ng kh√≠ ·ªü H√† N·ªôi trong giai ƒëo·∫°n **{kpi['date_range']}**.  
**Quy m√¥ d·ªØ li·ªáu.** {kpi['n']:,} quan s√°t gi·ªù.  
**M·ª©c √¥ nhi·ªÖm ƒë√°ng ch√∫ √Ω.** PM2.5 cao nh·∫•t ƒë·∫°t **{kpi['max_pm25']:.1f} ¬µg/m¬≥** v√†o **{kpi['max_pm25_time']}**.  
**R·ªßi ro ph∆°i nhi·ªÖm.** T·ªâ l·ªá gi·ªù c√≥ AQI **t·ª´ m·ª©c Poor tr·ªü l√™n** ~ **{kpi['bad_hours_share']*100:.1f}%**.  
**Ng√†y t·ªá nh·∫•t (theo PM2.5 trung b√¨nh ng√†y).** **{kpi['top_day']}**.  

**Insight nhanh:**
- **Nh·ªãp ng√†y‚Äìƒë√™m:** So s√°nh PM2.5 theo gi·ªù cho th·∫•y c√°c ƒë·ªânh th∆∞·ªùng r∆°i v√†o bu·ªïi **s√°ng s·ªõm** v√†/ho·∫∑c **t·ªëi**, g·ª£i √Ω ·∫£nh h∆∞·ªüng giao th√¥ng ‚Äì ngh·ªãch nhi·ªát.  
- **T∆∞∆°ng quan PM2.5‚ÄìPM10:** H·ªá s·ªë t∆∞∆°ng quan cao ‚Üí ngu·ªìn h·∫°t m·ªãn v√† th√¥ c√πng bi·∫øn thi√™n; **pm_ratio** gi√∫p nh·∫≠n di·ªán ∆∞u th·∫ø h·∫°t m·ªãn.  
- **ƒê·ªô tr·ªÖ 24h:** Trung b√¨nh tr∆∞·ª£t 24h l√†m m∆∞·ª£t bi·∫øn ƒë·ªông, h·ªØu √≠ch ƒë·ªÉ c·∫£nh b√°o s·ªõm n·∫øu xu h∆∞·ªõng tƒÉng k√©o d√†i.

> L∆∞u √Ω: N·∫øu b·∫°n d√πng g√≥i OWM tr·∫£ ph√≠, d·ªØ li·ªáu 30 ng√†y s·∫Ω ƒë·∫ßy ƒë·ªß h∆°n; b·∫£n hi·ªán t·∫°i c√≥ th·ªÉ b·ªã gi·ªõi h·∫°n ‚â§5 ng√†y.
""")

# =========================
# CHARTS ‚Äî INTERACTIVE (‚â•3)
# =========================

# 1) Line/Area theo th·ªùi gian (Plotly)
st.subheader("üìà PM2.5 & PM10 theo th·ªùi gian")
fig_area = go.Figure()
fig_area.add_trace(go.Scatter(x=df["time"], y=df["pm2_5"], mode="lines", name="PM2.5", fill="tozeroy"))
fig_area.add_trace(go.Scatter(x=df["time"], y=df["pm10"], mode="lines", name="PM10", fill=None))
fig_area.update_layout(xaxis_title="Time", yaxis_title="¬µg/m¬≥", hovermode="x unified")
st.plotly_chart(fig_area, use_container_width=True)

# 2) Histogram/Box/Violin (Plotly)
st.subheader("üì¶ Ph√¢n ph·ªëi PM2.5")
tab_hist, tab_box, tab_violin = st.tabs(["Histogram", "Boxplot", "Violin"])
with tab_hist:
    st.plotly_chart(px.histogram(df, x="pm2_5", nbins=40, marginal="rug", title="Histogram PM2.5"), use_container_width=True)
with tab_box:
    st.plotly_chart(px.box(df, y="pm2_5", points="all", title="Boxplot PM2.5"), use_container_width=True)
with tab_violin:
    st.plotly_chart(px.violin(df, y="pm2_5", box=True, points="all", title="Violin PM2.5"), use_container_width=True)

# 3) Scatter + h·ªìi quy (Plotly trendline=ols)
st.subheader("üî¨ PM2.5 vs PM10 (Scatter + OLS)")
trend = "ols" if show_regression else None
scatter_fig = px.scatter(df, x="pm2_5", y="pm10", opacity=0.7,
                         trendline=trend, trendline_color_override="black",
                         labels={"pm2_5":"PM2.5 (¬µg/m¬≥)", "pm10":"PM10 (¬µg/m¬≥)"},
                         hover_data=["time"])
st.plotly_chart(scatter_fig, use_container_width=True)

# 4) Heatmap t∆∞∆°ng quan
st.subheader("üß™ T∆∞∆°ng quan c√°c ch·ªâ ti√™u")
corr = corr_df(df)
heat = px.imshow(corr, text_auto=True, aspect="auto", title="Correlation Heatmap")
st.plotly_chart(heat, use_container_width=True)

# 5) Sunburst/Treemap
st.subheader("ü™Ö Ph√¢n c·∫•p AQI")
if use_treemap.startswith("Sunburst"):
    sb = px.sunburst(df, path=["aqi_label","weekday"], values=None, title="Sunburst: AQI ‚Üí Weekday")
    st.plotly_chart(sb, use_container_width=True)
else:
    # Treemap AQI ‚Üí Hour buckets
    df["hour_bin"] = pd.cut(df["hour"], bins=[-0.1,6,12,18,24], labels=["0‚Äì6","6‚Äì12","12‚Äì18","18‚Äì24"])
    tm = px.treemap(df, path=["aqi_label","hour_bin"], title="Treemap: AQI ‚Üí Hour")
    st.plotly_chart(tm, use_container_width=True)

# 6) B·∫£n ƒë·ªì (Folium ho·∫∑c Mapbox)
st.subheader("üó∫Ô∏è B·∫£n ƒë·ªì")
if map_mode.startswith("Folium"):
    # T·∫°o geojson th·ªùi gian ‚Äî 1 ƒëi·ªÉm/gi·ªù, b√°n k√≠nh theo PM2.5
    m = folium.Map(location=[lat, lon], zoom_start=11, tiles="cartodbpositron")
    # thang m√†u theo PM2.5
    cm = linear.YlOrRd_09.scale(df["pm2_5"].min(), df["pm2_5"].max())
    features = []
    for _, r in df.iterrows():
        t_iso = r["time"].isoformat()
        pm = float(r["pm2_5"])
        rad = max(50, min(500, pm*5))  # b√°n k√≠nh theo pm2.5
        color = cm(pm)
        feat = {
            "type": "Feature",
            "geometry": {"type":"Point","coordinates":[r["lon"], r["lat"]]},
            "properties": {
                "time": t_iso,
                "style": {"color": color, "fillColor": color, "opacity":0.7, "fillOpacity":0.4},
                "icon": "circle",
                "popup": f"{t_iso}<br>PM2.5: {pm:.1f} ¬µg/m¬≥"
            }
        }
        features.append(feat)
    gj = {
        "type": "FeatureCollection",
        "features": features
    }
    TimestampedGeoJson(
        data=gj,
        period="PT1H",
        duration="PT1H",
        add_last_point=True,
        auto_play=False,
        loop=False
    ).add_to(m)
    cm.add_to(m)
    st.components.v1.html(m._repr_html_(), height=520, scrolling=False)
else:
    # Plotly mapbox scatter ‚Äî c·∫ßn mapbox token n·∫øu mu·ªën style map ƒë·∫πp, m·∫∑c ƒë·ªãnh v·∫´n ch·∫°y
    mb = px.scatter_mapbox(df, lat="lat", lon="lon", size="pm2_5",
                           color="pm2_5", hover_name="time",
                           zoom=10, height=520)
    mb.update_layout(mapbox_style="open-street-map", margin=dict(l=0,r=0,t=0,b=0))
    st.plotly_chart(mb, use_container_width=True)

# =========================
# WORDCLOUD ho·∫∑c NETWORK GRAPH
# =========================
st.subheader("‚òÅÔ∏è WordCloud t·ª´ nh√£n AQI (l·∫∑p theo t·∫ßn su·∫•t)")
wc_img = make_wordcloud(df["aqi_label"])
if wc_img is not None:
    st.image(wc_img, caption="WordCloud v·ªÅ nh√£n AQI", use_column_width=True)
else:
    st.info("Kh√¥ng ƒë·ªß d·ªØ li·ªáu vƒÉn b·∫£n ƒë·ªÉ t·∫°o WordCloud.")

# =========================
# EXTRA: B·∫¢NG D·ªÆ LI·ªÜU & T·∫¢I XU·ªêNG
# =========================
st.subheader("üìÑ D·ªØ li·ªáu ƒë√£ chu·∫©n ho√°")
st.dataframe(df.tail(500), use_container_width=True)

@st.cache_data
def to_csv_bytes(df_in):
    return df_in.to_csv(index=False).encode("utf-8")

st.download_button("‚¨áÔ∏è T·∫£i CSV", data=to_csv_bytes(df), file_name="hanoi_air_quality.csv", mime="text/csv")

# =========================
# FOOTER NOTES
# =========================
st.caption("""
- **X·ª≠ l√Ω thi·∫øu**: n·ªôi suy theo th·ªùi gian & forward/backward fill (ƒëi·ªÉm ƒëo c·ªë ƒë·ªãnh).
- **ƒê·∫∑c tr∆∞ng m·ªõi**: `pm_ratio` (PM2.5/PM10), `rolling_pm25_24h`, `rolling_pm10_24h`, nh√£n `aqi_label`, bi·∫øn th·ªùi gian (gi·ªù, weekday, weekend).
- **H·∫°n ch·∫ø API**: N·∫øu g√≥i OWM kh√¥ng cho ƒë·ªß 30 ng√†y, ·ª©ng d·ª•ng v·∫´n ch·∫°y v·ªõi ph·∫ßn d·ªØ li·ªáu hi·ªán c√≥.
""")
